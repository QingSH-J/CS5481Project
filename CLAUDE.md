# this is a course project, Knowledge based system

## Knowledge Base Q&A System

this is project requierment

LLMs are powerful, but they lack knowledge of specific, private, or recent documents. This project tasks you with
building a Question-Answering system based on a custom knowledge base, using the Retrieval-Augmented Generation
(RAG) pattern. This is a core data engineering task that enables LLMs to use external data.
The system should be able to answer questions based on a specific set of documents you provide. The main
challenge is not the chatbot interface, but the backend data pipeline that prepares and serves the knowledge for retrieval.
Requirements:
1. Data Preparation: Collect a set of documents (e.g., course notes, product manuals, company reports). Write
a script to load, chunk, and create vector embeddings for these documents using an open-source embedding
model or API.
2. Knowledge Base Storage: Store the document chunks and their corresponding embeddings in a vector database.
3. Retrieval Mechanism: Given a userâ€™s question, first convert the question to an embedding, then retrieve the
most relevant document chunks from the vector database based on semantic similarity.
4. Answer Generation: Pass the original question and the retrieved document chunks to an LLM API. Prompt the
LLM to generate an answer based *only* on the provided context.
Reference
1. Lewis, P., et al. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in
Neural Information Processing Systems, 33.
2. LangChain Documentation. (2024). Question Answering over Documents.


# What to do?

Phase 1: The Technology StackTo keep this efficient and standard, I recommend the following stack:Orchestration: LangChain (Standard for RAG workflows).Embeddings: OpenAI Embeddings (High quality) or HuggingFace (Free/Local).Vector Database: ChromaDB (Open-source, runs locally, easy to set up).LLM: OpenAI (GPT-3.5/4) or Ollama (if you need a fully local model).

Phase 2: Implementation GuideHere is the complete logic broken down into the requirements you listed.

Requirement 1 & 2: Data Preparation & StorageYou need to load text, split it into digestible "chunks" (so you don't hit token limits), embed them into vector space, and store them.
Key Concept: Chunking If you feed an entire PDF into an embedding model, you lose nuance. You want to split text into chunks (e.g., 1000 characters) with an overlap (e.g., 200 characters). The overlap ensures that context isn't lost if a sentence is cut in half between chunks.

Requirement 3 & 4: Retrieval & Answer Generation
When a user asks a question:Embed the question.Perform a Cosine Similarity Search in the DB.Retrieve the top $k$ (usually 3-5) chunks.Stuff these chunks into a prompt.